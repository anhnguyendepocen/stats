\documentclass[11pt]{article}

\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{ifthen}
\usepackage{epsfig}
\usepackage{fancyvrb}
\usepackage{fancyhdr}
\usepackage[utf8x]{inputenc}
\usepackage{url}

\usepackage[top=1.25in, bottom=1.25in, left=1.25in, right=1.25in]{geometry}

\pagestyle{fancy}
\fancyhead{} \fancyfoot{} \lhead{\nouppercase{Intro Stats R}}
\rhead{\nouppercase{\leftmark}} \cfoot{\thepage}

\title{One Way ANOVA\\Introduction to Statistics Using R (Psychology 9041B)}

\author{Paul Gribble}

\usepackage[iso]{datetime}
\usdate

\date{Winter, 2016}

\SweaveOpts{keep.source=TRUE}

\setlength{\skip\footins}{20mm}

\begin{document}

\maketitle

\section{One-way between subjects ANOVA}

Assume we have collected data from 15 subjects, each of whom were
randomly assigned to one of three groups:

\begin{center}
	\begin{tabular}{c|c|c}
		group1 &group2 &group3 \\
		\hline
		4 &7 &6 \\
		5 &4 &9 \\
		2 &6 &8 \\
		1 &3 &5 \\
		3 &5 &7 \\
	\end{tabular}
\end{center}

The single factor between subjects analysis of variance (ANOVA) tests
the null hypothesis that the means of the populations from which the
three samples were drawn, are the same.

\begin{center}
\begin{tabular}{ll}
	$H_{0}$: &$\mu_{1} = \mu_{2} = \mu_{3}$ \\
	$H_{1}$: &$\mu_{1} \neq \mu_{2} \neq \mu_{3}$
\end{tabular}
\end{center}

One way of thinking about the ANOVA is that it partitions the total
variance in the dependent variable into two parts: between-groups
variance (variance due to differences between groups) and within-group
variance (the variability within a group). The F-test is then a test
of whether the between-groups variance is significantly greater than
the within-groups variance --- in other words, are the observed
differences larger than what one would expect given the typical
variability within a group?

The other way of thinking about the ANOVA is using a model comparison
approach. Under a \emph{restricted model}, one seeks to account for
the dependent variable using a single parameter - the grand
mean. Under a \emph{full model}, one introduces additional parameters
allowing one to adjust the value of the dependent variable depending
on group membership:

\begin{eqnarray}
	m_{restricted}: &Y_{ij} &= \mu + \epsilon_{ij} \\
	m_{full}: &Y_{ij} &= \mu + \alpha_{j} + \epsilon_{ij}
\end{eqnarray}

Which model fits the data better? Of course, the full model will fit
the data better, as it has more parameters (and hence more
flexibility). The real question is, whether the increase in model fit
(or the decrease in model error, or residual), is \emph{worth} giving
up the degrees of freedom inherent in having to estimate additional
parameters? This is the question that the F-test in the ANOVA answers.

In the above example, the restricted model postulates that the data
can be fit using a single parameter, the grand mean $\mu$. The full
model postulates that the data should be fit using three parameters,
$\mu_{1}$, $\mu_{2}$ and $\mu_{3}$ --- i.e. a different mean for each
group.

We can represent these two models graphically. The restricted model
assumes all the data are fit by a single parameter, the grand mean
$\mu$ (The vertical dashed lines indicate the model prediction
errors):

<<>>=
Y <- c(4,5,2,1,3,7,4,6,3,5,6,9,8,5,7)
myFac <- c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3)
plot(Y, pch=myFac, main="restricted model")
abline(h=mean(Y))
for (i in 1:length(Y)) {
	lines(c(i,i), c(Y[i], mean(Y)), lty=2)
}
@

<<echo=false,fig=true>>=
plot(Y, pch=myFac, main="restricted model")
abline(h=mean(Y))
for (i in 1:length(Y)) {
	lines(c(i,i), c(Y[i], mean(Y)), lty=2)
}
@

Under the full model, we estimate a different mean for each group:

<<>>=
Y <- c(4,5,2,1,3,7,4,6,3,5,6,9,8,5,7)
myFac <- c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3)
plot(Y, pch=myFac, main="full model")
for (j in 1:3) {
	w <- which(myFac==j)
	lines(c(min(w),max(w)),c(mean(Y[w]),mean(Y[w])))
	for (i in 1:length(w)) {
		lines(c(w[i],w[i]), c(Y[w[i]], mean(Y[w])), lty=2)
	}
}
@

<<echo=false,fig=true>>=
plot(Y, pch=myFac, main="full model")
for (j in 1:3) {
	w <- which(myFac==j)
	lines(c(min(w),max(w)),c(mean(Y[w]),mean(Y[w])))
	for (i in 1:length(w)) {
		lines(c(w[i],w[i]), c(Y[w[i]], mean(Y[w])), lty=2)
	}
}
@

Again, the dashed vertical lines indicate model error. Obviously the
full model predicts the data better. The question ANOVA will answer
is, whether the increase in model fit (the decrease in prediction
error) is worth giving up the degrees of freedom necessary to estimate
the additional parameters.

In \texttt{R} it's very simple to perform an ANOVA using the
\texttt{aov} function:

<<>>=
m1 <- aov(Y ~ factor(myFac))
summary(m1)
@

In this case the main effect of \texttt{myFac} is significant at
$p=0.006196$, so we would reject the null hypothesis that the three
groups were sampled from the same population (or populations with the
same mean).

Another way of running the anova that highlights the fact that we are
fitting three parameters, is to use the \texttt{lm()} function:

<<>>=
m2 <- lm(Y ~ factor(myFac))
summary(m2)
@

The parameter estimates are called \texttt{Coefficients} and are
listed in the column marked \texttt{Estimate}. In this case the
estimate for the first group (called \texttt{Intercept} in the anova
output) is $3.0000$. The estimate for the mean of group two is equal
to the \texttt{Intercept} plus $2.0000$, which equals
$5.0000$. Likewise the estimate for group three is $3.0000$ + $4.0000$
which equals $7.0000$.

We can then perform an F-test by applying the \texttt{anova()} command
to the model object \texttt{m2}:

<<>>=
anova(m2)
@

The F-test of the main effect of the factor is called an
\emph{omnibus} test. A significant test indicates only that the
population means are not equal --- we would need to perform follow-up
tests to find out specifically which groups differ. This topic will be
covered in the next chapter.

\subsection*{Testing Assumptions}

Two testable assumptions of ANOVA are homogeneity of variances (the
variances in each group are the same) and normality (the data within
each group are normally distributed).

\subsubsection*{Normality}

To test the normality assumption we can use the Shapiro-Wilk normality
test. In \texttt{R} the function is \texttt{shapiro.test()}:

<<>>=
ferrydata=read.table("../data/ferrydata.csv", header=T, sep=",")
shapiro.test(subset(ferrydata$Passengers, ferrydata$Day=="Fri"))
shapiro.test(subset(ferrydata$Passengers, ferrydata$Day=="Sat"))
shapiro.test(subset(ferrydata$Passengers, ferrydata$Day=="Sun"))
@ 

ANOVA is generally robust to violations of normality, as long as all
groups violate from normality in the same way, and as long as the
number of observations in each group is the same.

If there is a violation of the normality assumption and you are
concerned about inflated Type-I error rates, one approach is to apply
a transformation to the data to make it normal. Some common
transformations include square-root, logarithm, reciprocal,
inverse-sine (see text). The tradeoff is that although these
mathematical transformations may fix non-normality, you must keep in
mind that conclusions based on transformed data only apply to the
transformed data, not necessarily to the original data. This can make
interpretation difficult.

\subsubsection*{Homogeneity of Variances}

To test homogeneity of variances we can use the bartlett test, in
\texttt{R} the function is \texttt{bartlett.test()}:

<<>>=
ferrydata=read.table("../data/ferrydata.csv", header=T, sep=",")
bartlett.test(Passengers ~ Day, data=ferrydata)
@

If there is a violation of homogeneity of variance, then one approach
is to use the Welch correction (as described in your text), which
adjusts the degrees of freedom to compensate for the unequal
variances. In \texttt{R} you can do this using the
\texttt{oneway.test()} function.

ANOVA is generally robust to violations of homogeneity of variances,
as long as sample sizes are equal, and as long as the normality
assumption holds.

\subsection*{Graphics}

There are many ways to plot your data. Some common plotting functions
that are included in the base distribution of R are:

\begin{itemize}
	\item \texttt{plot()}
	\item \texttt{boxplot()}
	\item \texttt{barplot()}
\end{itemize}

I suggest you look at the R help files for each function, and the
example code at the bottom of each help file, to see how these work.

There is a more powerful graphics package you can add to R called
\texttt{ggplot2}. To download and install it into R issue the
following command in R:

\begin{quote}
	\texttt{install.packages("ggplot2")}
\end{quote}

Then each time you launch R and you wish to use the package type:

\begin{quote}
	\texttt{library(ggplot2)}
\end{quote}

There is lots of documentation about the \texttt{ggplot2} package
online, I suggest doing a google search. The homepage is:
\url{http://had.co.nz/ggplot}.

\subsection*{an Example}

Let's say you have the following data:

<<>>=
dataURL <- "http://gribblelab.org/stats/data/ferrydata.csv"
ferrydata <- read.table(dataURL, header=T, sep=",")
ferrydata
@

You can generate a boxplot like this:

<<fig=true>>=
boxplot(Passengers ~ Day, data=ferrydata)
@

We can install and use the \texttt{gplots} package~\footnote{you'll
  need a one-time \texttt{install.packages("gplots")} to download and
  install the package, and then you'll need to issue the command
  \texttt{library(gplots)} once each time you start R, to use it} to
do more traditional looking plots:

<<echo=false,results=hide>>=
library(gplots)
@

<<fig=true>>=
plotmeans(Passengers ~ Day, data=ferrydata)
@

This shows means and 95~\% confidence intervals. If we want to plot
some other quantity instead of confidence intervals, for example
standard errors of the mean, we can do it by feeding the desired
values into the \texttt{plotCI} function. We are also going to use the
\texttt{split()} function to split our data table into groups, and the
\texttt{sapply()} function to apply a function to each part of the
array produced by \texttt{split()}.

<<fig=true>>=
tmp <- split(ferrydata$Passengers, ferrydata$Day)
tmp
means <- sapply(tmp, mean)
means
n <- sapply(tmp, length)
n
stdev <- sqrt(sapply(tmp, var))
stdev
se <- stdev / sqrt(n)
se
plotCI(x = means, uiw = se, type="b", ylab="Passengers", xlab="Day", xaxt="n")
axis(side=1, at=1:3, levels(ferrydata$Day))
@

\end{document}

